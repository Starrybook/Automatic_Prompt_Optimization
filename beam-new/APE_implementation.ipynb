{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 思路：束搜索，束大小为 B ，每次迭代选取前 b 个最优指令，对错例进行 s - 1 次去重采样，经过反思和细化，生成新指令，满足 b * s = B ，得到新束\n",
    "# 过程中保留上一次迭代的最优指令，原因是每一次反思与细化的结果不稳定，可能会导致性能下降，因此需要保留上一次迭代的最优指令\n",
    "def APE_beam(eval_file_path, iter_limit, B, b, s):\n",
    "    # prompt_insts = [[{\"inst\": initial prompt_inst, \"accuracy\": 0.0, \"responses_path\": \"\"} * B]]\n",
    "    prompt_insts = [get_initial_prompt_insts(B)]\n",
    "    best_prompt_inst = {\"inst\": \"\", \"accuracy\": 0.0, \"responses_path\": \"\"}\n",
    "    for i in range(iter_limit):\n",
    "        for inst_dict in prompt_insts[i]:\n",
    "            if inst_dict[\"responses_path\"] == \"\":\n",
    "                responses = get_responses(inst_dict[\"inst\"], dataset)\n",
    "                store responses to file and set inst_dict[\"responses_path\"]\n",
    "                inst_dict[\"accuracy\"] = calculate_accuracy(responses)\n",
    "                update best_prompt_inst if needed\n",
    "        if best_prompt_inst[\"accuracy\"] == 1.0:\n",
    "            break\n",
    "        Record indexes of top b prompt_insts\n",
    "        prompt_insts.append([])\n",
    "        for index in top_b_indexes:\n",
    "            prompt_insts[i+1].append(prompt_insts[i][index])\n",
    "            all_error_examples = [select error examples from dataset]\n",
    "            error_example_sets = [sample s - 1 times from all_error_examples, and remove duplicates]\n",
    "            for error_example_set in error_example_sets:\n",
    "                reflection_prompt = get_reflection_prompt(prompt_insts[i][index], error_example_set)\n",
    "                reasons = reflect(reflection_prompt)\n",
    "                refinement_prompt = get_refinement_prompt(prompt_insts[i][index], error_example_set, reasons)\n",
    "                prompt_insts[i+1].append({\"inst\": refine(refinement_prompt), \"accuracy\": 0.0})\n",
    "    return best_prompt_inst\n",
    "\"\"\"\n",
    "\"\"\" Globals \"\"\"\n",
    "\"\"\" for APE_utils \"\"\"\n",
    "TARGET_MODEL = \"gpt-3.5-turbo\"\n",
    "OPTIMIZER_MODEL = \"gpt-3.5-turbo\"\n",
    "\"\"\" for data_process \"\"\"\n",
    "TASK_CODE_NAME = 'beam-trial-8'\n",
    "MOVIE_RESPONSES_DIR = \"../data/response/movie\" + '/' + TASK_CODE_NAME\n",
    "GSM8K_RESPONSES_DIR = \"../data/response/gsm8k\" + '/' + TASK_CODE_NAME\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "from APE_utils import *\n",
    "\n",
    "# CURRENT_DATASET = \"movie\"\n",
    "CURRENT_DATASET = \"gsm8k\"\n",
    "# eval_file_path = '../data/movie/eval.json'\n",
    "eval_file_path = '../data/gsm8k/eval-1.json'\n",
    "# if not os.path.exists(MOVIE_RESPONSES_DIR):\n",
    "#     os.makedirs(MOVIE_RESPONSES_DIR)\n",
    "if not os.path.exists(GSM8K_RESPONSES_DIR):\n",
    "    os.makedirs(GSM8K_RESPONSES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_limit = 4\n",
    "B = 2\n",
    "b = 2\n",
    "s = 3\n",
    "\n",
    "# def APE_beam(eval_file_path: str, iter_limit: int, B = 8, b = 2, s = 3):\n",
    "# set globals\n",
    "FEEDBACK_REASONS_NUM = 2\n",
    "EVAL_SAMPLE_NUM = 50\n",
    "ERROR_SAMPLE_NUM = 3\n",
    "# init\n",
    "eval_set = gen_samples_from_dataset(eval_file_path, EVAL_SAMPLE_NUM, keep_orginal_order=False)\n",
    "prompt_insts = [get_initial_prompt_insts(CURRENT_DATASET, B)]   # 之后扩充一下\n",
    "best_prompt_inst = {\"inst\": \"\", \"accuracy\": 0.0, \"responses_path\": \"\", \"error_num\": 0}\n",
    "reflection_refinement_record = []   # [[(reflection_prompt, reasons, refinement_prompt, improved prompts),...],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iter_limit):\n",
    "    if DEBUG:\n",
    "        print(f\"\\n\\n>>> Current iteration: {i}\")\n",
    "    for inst_dict, inst_dict_id in zip(prompt_insts[i], range(len(prompt_insts[i]))):\n",
    "        if inst_dict[\"responses_path\"] == \"\":\n",
    "            responses = get_target_model_responses(\n",
    "                CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR, \n",
    "                eval_set, inst_dict[\"inst\"], if_print=False\n",
    "            )\n",
    "            inst_dict[\"responses_path\"] = write_target_model_responses(\n",
    "                TARGET_MODEL + f\"_{i}-{inst_dict_id}\",\n",
    "                MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR, \n",
    "                responses\n",
    "            )\n",
    "            inst_dict[\"accuracy\"] = evaluation_movie(CURRENT_DATASET, inst_dict[\"responses_path\"]) \\\n",
    "                                    if CURRENT_DATASET == 'movie' \\\n",
    "                                    else evaluation_gsm8k(CURRENT_DATASET, inst_dict[\"responses_path\"])\n",
    "            if inst_dict[\"accuracy\"] > best_prompt_inst[\"accuracy\"]:\n",
    "                best_prompt_inst = inst_dict\n",
    "        if DEBUG:\n",
    "            print(f\">>> Current prompt instance, iteration {i}, id {inst_dict_id}\")\n",
    "            print(f\">>> Current accuracy rate: {inst_dict['accuracy']}\")\n",
    "    if best_prompt_inst[\"accuracy\"] == 1.0:\n",
    "        print(f\"Early stop at iteration {i}\")\n",
    "        break\n",
    "    # sort\n",
    "    top_b_indexes = []\n",
    "    for _ in range(b):\n",
    "        max_accuracy = 0.0\n",
    "        max_accuracy_index = -1\n",
    "        for inst_dict_id in range(len(prompt_insts[i])):\n",
    "            if inst_dict_id not in top_b_indexes and prompt_insts[i][inst_dict_id][\"accuracy\"] > max_accuracy:\n",
    "                max_accuracy = prompt_insts[i][inst_dict_id][\"accuracy\"]\n",
    "                max_accuracy_index = inst_dict_id\n",
    "        top_b_indexes.append(max_accuracy_index)\n",
    "    if DEBUG:\n",
    "        print(f\"\\n>>> Top b indexes in this iteration: {top_b_indexes}\")\n",
    "        print(\"Now begin to generate new prompt instances...\")\n",
    "    reflection_refinement_record.append([])\n",
    "    prompt_insts.append([])\n",
    "    # sample and update\n",
    "    for index in top_b_indexes:\n",
    "        # if DEBUG:\n",
    "        #     print(f\">>> for index {index} in top b indexes:\")\n",
    "        prompt_insts[i+1].append(prompt_insts[i][index].copy())\n",
    "        error_example_sets = get_error_example_sets_movie(prompt_insts[i][index][\"responses_path\"], ERROR_SAMPLE_NUM, s - 1) \\\n",
    "                            if CURRENT_DATASET == 'movie' \\\n",
    "                            else get_error_example_sets_gsm8k(prompt_insts[i][index][\"responses_path\"], ERROR_SAMPLE_NUM, s - 1)\n",
    "        for error_example_set, set_index in zip(error_example_sets, range(len(error_example_sets))):\n",
    "            # if DEBUG:\n",
    "            #     print(f\">>> for error example set {set_index}:\")\n",
    "            reflection_prompt = gen_reflection(FEEDBACK_REASONS_NUM, prompt_insts[i][index][\"inst\"], error_example_set)\n",
    "            reasons = get_reflection_from_optimizer(OPTIMIZER_MODEL, reflection_prompt)\n",
    "            refinement_prompt = gen_refinement(prompt_insts[i][index][\"inst\"], error_example_set, reasons)\n",
    "            improved_inst = get_refinement_from_optimizer(OPTIMIZER_MODEL, refinement_prompt)[0]\n",
    "            prompt_insts[i+1].append({\"inst\": improved_inst, \"accuracy\": 0.0, \"responses_path\": \"\"})\n",
    "            reflection_refinement_record[i].append((reflection_prompt, reasons, refinement_prompt, improved_inst))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print prompt_insts\n",
    "print(f\"Prompt instances:\")\n",
    "for iter_list, iter_index in zip(prompt_insts, range(len(prompt_insts))):\n",
    "    print(f\"In iteration {iter_index}: \")\n",
    "    for inst_dict, dict_index in zip(iter_list, range(len(iter_list))):\n",
    "        print(f\">>> {dict_index}\")\n",
    "        print(f\">>> Prompt: {inst_dict['inst']}\")\n",
    "        print(f\">>> Accuracy: {inst_dict['accuracy']}\")\n",
    "        print(f\">>> Responses path: {inst_dict['responses_path']}\")\n",
    "    print(\"=\"*50)\n",
    "print(f\"Best prompt instance: {best_prompt_inst}\")\n",
    "record_save_path = MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR\n",
    "record_save_path += \"/record.txt\"\n",
    "with open(record_save_path, \"w\") as f:\n",
    "    for i in range(len(reflection_refinement_record)):\n",
    "        f.write(f\"\\n\\n>>> Iteration {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for record in reflection_refinement_record[i]:\n",
    "            f.write(f\">>> Reflection prompt: \\n{record[0]}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\">>> Reasons: \\n{record[1]}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\">>> Refinement prompt: \\n{record[2]}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\">>> Improved prompt: \\n{record[3]}\")\n",
    "            f.write(\"\\n\")\n",
    "# return best_prompt_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每一次迭代中所有指令的正确率\n",
    "accuracy_list = []\n",
    "for iter_list in prompt_insts:\n",
    "    accuracy_list.append([inst_dict[\"accuracy\"] for inst_dict in iter_list])\n",
    "# 打印时保留 3 位小数\n",
    "print(\"Accuracy list:\")\n",
    "for accuracy_list_item in accuracy_list:\n",
    "    print([round(accuracy, 3) for accuracy in accuracy_list_item])\n",
    "    print(\"average: \", round(sum(accuracy_list_item) / len(accuracy_list_item), 3), \"; max: \", round(max(accuracy_list_item), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "从 ../data/gsm8k/test.jsonl 中载入数据；\n",
    "从 ../data/gsm8k/eval.json 中载入数据；\n",
    "对 test.jsonl 中的数据进行采样，得到 100 个样本，确保与 eval.json 中的数据不重复；\n",
    "重复的判断标准是 question 字段相同\n",
    "将采样结果写入 ../data/gsm8k/eval-1.json 中\n",
    "\"\"\"\n",
    "# eval_file_path = '../data/gsm8k/eval.json'\n",
    "# test_file_path = '../data/gsm8k/test.jsonl'\n",
    "# with open(test_file_path, \"r\") as f:\n",
    "#     test_set = [json.loads(line) for line in f.readlines()]\n",
    "#     with open(eval_file_path, \"r\") as f:\n",
    "#         eval_set = json.load(f)\n",
    "#         random.shuffle(test_set)\n",
    "#         new_eval_set = []\n",
    "#         while len(new_eval_set) < 100:\n",
    "#             new_item = test_set.pop()\n",
    "#             question = new_item[\"question\"]\n",
    "#             flag = False\n",
    "#             for eval_item in eval_set:\n",
    "#                 if eval_item[\"question\"] == question:\n",
    "#                     flag = True\n",
    "#                     break\n",
    "#             if not flag:\n",
    "#                 new_eval_set.append(new_item)\n",
    "#         with open('../data/gsm8k/eval-1.json', \"w\") as f:\n",
    "#             json.dump(new_eval_set, f, indent=4)\n",
    "#             print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test \"\"\"\n",
    "test_file_path = '../data/gsm8k/eval.json'\n",
    "TEST_SAMPLE_NUM = 50\n",
    "test_set = gen_samples_from_dataset(test_file_path, TEST_SAMPLE_NUM, keep_orginal_order=True)\n",
    "test_responses = get_target_model_responses(\n",
    "    CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR, \n",
    "    test_set, best_prompt_inst[\"inst\"], if_print=False\n",
    ")\n",
    "test_responses_path = write_target_model_responses(\n",
    "    TARGET_MODEL + \"_test\",\n",
    "    MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR, \n",
    "    test_responses\n",
    ")\n",
    "accuracy_rate = evaluation_gsm8k(CURRENT_DATASET, test_responses_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pteng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
