{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\"\"\" Globals \"\"\"\n",
    "TARGET_MODEL = \"gpt-3.5-turbo\"\n",
    "OPTIMIZER_MODEL = \"gpt-3.5-turbo\"\n",
    "TASK_CODE_NAME = 'summary-greedy-2'\n",
    "MOVIE_RESPONSES_DIR = \"../data/response/movie\" + '/' + TASK_CODE_NAME\n",
    "GSM8K_RESPONSES_DIR = \"../data/response/gsm8k\" + '/' + TASK_CODE_NAME\n",
    "DEBUG = True\n",
    "CURRENT_DATASET = \"gsm8k\"\n",
    "eval_file_path = '../data/gsm8k/eval.json'\n",
    "if not os.path.exists(GSM8K_RESPONSES_DIR):\n",
    "    os.makedirs(GSM8K_RESPONSES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_limit = 10\n",
    "FEEDBACK_REASONS_NUM = 2\n",
    "EVAL_SAMPLE_NUM = 50\n",
    "ERROR_SAMPLE_NUM = 3\n",
    "# init\n",
    "random.seed(2)\n",
    "eval_set = gen_samples_from_dataset(eval_file_path, EVAL_SAMPLE_NUM, keep_orginal_order=False)\n",
    "prompt_insts = get_initial_prompt_insts(CURRENT_DATASET, 1)\n",
    "best_prompt_inst = {\"inst\": \"\", \"accuracy\": 0.0, \"responses_path\": \"\"}\n",
    "reflection_refinement_record = []   # [[(reflection_prompt, reasons, refinement_prompt, improved prompts),...],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iter_limit):\n",
    "    if DEBUG:\n",
    "        print(f\"\\n\\n>>> Current iteration: {i}\")\n",
    "    responses = get_target_model_responses(\n",
    "        CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR,\n",
    "        eval_set, prompt_insts[-1][\"inst\"], if_print=False\n",
    "    )\n",
    "    prompt_insts[-1][\"responses_path\"] = write_target_model_responses(\n",
    "        TARGET_MODEL + f\"_{i}\", \n",
    "        MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR,\n",
    "        responses\n",
    "    )\n",
    "    prompt_insts[-1][\"accuracy\"] = evaluation_gsm8k(CURRENT_DATASET, prompt_insts[-1][\"responses_path\"])\n",
    "    if prompt_insts[-1][\"accuracy\"] > best_prompt_inst[\"accuracy\"]:\n",
    "        best_prompt_inst = prompt_insts[-1]\n",
    "    if DEBUG:\n",
    "        print(f\">>> Current accuracy rate: {prompt_insts[-1]['accuracy']}\")\n",
    "    if best_prompt_inst[\"accuracy\"] == 1.0:\n",
    "        print(f\"Early stop at iteration {i}\")\n",
    "        break\n",
    "    error_example_set = get_error_example_sets_gsm8k(prompt_insts[-1][\"responses_path\"], ERROR_SAMPLE_NUM, 1)[0]\n",
    "    reflection_prompt = gen_reflection(FEEDBACK_REASONS_NUM, prompt_insts[-1][\"inst\"], error_example_set)\n",
    "    reasons = get_reflection_from_optimizer(OPTIMIZER_MODEL, reflection_prompt)\n",
    "    refinement_prompt = gen_refinement(prompt_insts[-1][\"inst\"], error_example_set, reasons)\n",
    "    improved_inst = get_refinement_from_optimizer(OPTIMIZER_MODEL, refinement_prompt)[0]\n",
    "    prompt_insts.append({\"inst\": improved_inst, \"accuracy\": 0.0, \"responses_path\": \"\"})\n",
    "    reflection_refinement_record.append((reflection_prompt, reasons, refinement_prompt, improved_inst))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prompt instances:\")\n",
    "for iter_inst, iter_index in zip(prompt_insts, range(len(prompt_insts))):\n",
    "    print(f\"In iteration {iter_index}: \")\n",
    "    print(f\">>> Prompt: {iter_inst['inst']}\")\n",
    "    print(f\">>> Accuracy: {iter_inst['accuracy']}\")\n",
    "    print(f\">>> Responses path: {iter_inst['responses_path']}\")\n",
    "print(\"=\"*50)\n",
    "print(\"Best prompt instance:\", best_prompt_inst)\n",
    "record_save_path = MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR\n",
    "record_save_path += \"/record.txt\"\n",
    "with open(record_save_path, \"w\") as f:\n",
    "    for record, index in zip(reflection_refinement_record, range(len(reflection_refinement_record))):\n",
    "        f.write(f\"\\n>>> Iteration {index}:\\n\")\n",
    "        f.write(f\">>> Reflection prompt: \\n{record[0]}\\n\")\n",
    "        f.write(f\">>> Reasons: \\n{record[1]}\\n\")\n",
    "        f.write(f\">>> Refinement prompt: \\n{record[2]}\\n\")\n",
    "        f.write(f\">>> Improved prompt: \\n{record[3]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy list:\")\n",
    "for inst in prompt_insts:\n",
    "    print(round(inst[\"accuracy\"], 3), end=\", \")\n",
    "print()\n",
    "print(\"average: \", round(sum([inst[\"accuracy\"] for inst in prompt_insts])/len(prompt_insts), 3))\n",
    "print(\"max: \", round(max([inst[\"accuracy\"] for inst in prompt_insts]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test \"\"\"\n",
    "test_file_path = '../data/gsm8k/test.json'\n",
    "TEST_SAMPLE_NUM = 100\n",
    "test_set = gen_samples_from_dataset(test_file_path, TEST_SAMPLE_NUM, keep_orginal_order=True)\n",
    "test_responses = get_target_model_responses(\n",
    "    CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR, \n",
    "    test_set, best_prompt_inst[\"inst\"], if_print=False\n",
    ")\n",
    "test_responses_path = write_target_model_responses(\n",
    "    TARGET_MODEL + \"_test\",\n",
    "    MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR, \n",
    "    test_responses\n",
    ")\n",
    "accuracy_rate = evaluation_gsm8k(CURRENT_DATASET, test_responses_path)\n",
    "print(f\"Accuracy rate on test set: {accuracy_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" initial test for comparison \"\"\"\n",
    "# test_file_path = '../data/gsm8k/test.json'\n",
    "# TEST_SAMPLE_NUM = 100\n",
    "# test_set = gen_samples_from_dataset(test_file_path, TEST_SAMPLE_NUM, keep_orginal_order=True)\n",
    "# responses = get_target_model_responses(\n",
    "#     CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR, \n",
    "#     test_set, prompt_insts[0][\"inst\"], if_print=False\n",
    "# )\n",
    "# responses_path = write_target_model_responses(\n",
    "#     TARGET_MODEL + \"_init-test\",\n",
    "#     MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR, \n",
    "#     responses\n",
    "# )\n",
    "# accuracy_rate = evaluation_gsm8k(CURRENT_DATASET, responses_path)\n",
    "# print(f\"Accuracy rate on test set with initial prompt: {accuracy_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pteng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
