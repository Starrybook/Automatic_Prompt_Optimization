{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\"\"\" Globals \"\"\"\n",
    "TARGET_MODEL = \"gpt-3.5-turbo\"\n",
    "OPTIMIZER_MODEL = \"gpt-3.5-turbo\"\n",
    "TASK_CODE_NAME = 'summary-beam-2'\n",
    "MOVIE_RESPONSES_DIR = \"../data/response/movie\" + '/' + TASK_CODE_NAME\n",
    "GSM8K_RESPONSES_DIR = \"../data/response/gsm8k\" + '/' + TASK_CODE_NAME\n",
    "DEBUG = True\n",
    "CURRENT_DATASET = \"gsm8k\"\n",
    "eval_file_path = '../data/gsm8k/eval.json'\n",
    "if not os.path.exists(GSM8K_RESPONSES_DIR):\n",
    "    os.makedirs(GSM8K_RESPONSES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_limit = 6\n",
    "B = 2\n",
    "b = 2\n",
    "s = 3\n",
    "FEEDBACK_REASONS_NUM = 2\n",
    "EVAL_SAMPLE_NUM = 50\n",
    "ERROR_SAMPLE_NUM = 3\n",
    "# init\n",
    "random.seed(2)\n",
    "eval_set = gen_samples_from_dataset(eval_file_path, EVAL_SAMPLE_NUM, keep_orginal_order=False)\n",
    "prompt_insts = [get_initial_prompt_insts(CURRENT_DATASET, B)]\n",
    "best_prompt_inst = {\"inst\": \"\", \"accuracy\": 0.0, \"responses_path\": \"\"}\n",
    "reflection_refinement_record = []   # [[(reflection_prompt, reasons, refinement_prompt, improved prompts),...],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iter_limit):\n",
    "    if DEBUG:\n",
    "        print(f\"\\n\\n>>> Current iteration: {i}\")\n",
    "    for inst_dict, inst_dict_id in zip(prompt_insts[i], range(len(prompt_insts[i]))):\n",
    "        if inst_dict[\"responses_path\"] == \"\":\n",
    "            responses = get_target_model_responses(\n",
    "                CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR, \n",
    "                eval_set, inst_dict[\"inst\"], if_print=False\n",
    "            )\n",
    "            inst_dict[\"responses_path\"] = write_target_model_responses(\n",
    "                TARGET_MODEL + f\"_{i}-{inst_dict_id}\",\n",
    "                MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR, \n",
    "                responses\n",
    "            )\n",
    "            inst_dict[\"accuracy\"] = evaluation_movie(CURRENT_DATASET, inst_dict[\"responses_path\"]) \\\n",
    "                                    if CURRENT_DATASET == 'movie' \\\n",
    "                                    else evaluation_gsm8k(CURRENT_DATASET, inst_dict[\"responses_path\"])\n",
    "            if inst_dict[\"accuracy\"] > best_prompt_inst[\"accuracy\"]:\n",
    "                best_prompt_inst = inst_dict\n",
    "        if DEBUG:\n",
    "            print(f\">>> Current prompt instance, iteration {i}, id {inst_dict_id}\")\n",
    "            print(f\">>> Current accuracy rate: {inst_dict['accuracy']}\")\n",
    "    if best_prompt_inst[\"accuracy\"] == 1.0:\n",
    "        print(f\"Early stop at iteration {i}\")\n",
    "        break\n",
    "    # sort\n",
    "    top_b_indexes = []\n",
    "    for _ in range(b):\n",
    "        max_accuracy = 0.0\n",
    "        max_accuracy_index = -1\n",
    "        for inst_dict_id in range(len(prompt_insts[i])):\n",
    "            if inst_dict_id not in top_b_indexes and prompt_insts[i][inst_dict_id][\"accuracy\"] > max_accuracy:\n",
    "                max_accuracy = prompt_insts[i][inst_dict_id][\"accuracy\"]\n",
    "                max_accuracy_index = inst_dict_id\n",
    "        top_b_indexes.append(max_accuracy_index)\n",
    "    if DEBUG:\n",
    "        print(f\"\\n>>> Top b indexes in this iteration: {top_b_indexes}\")\n",
    "        print(\"Now begin to generate new prompt instances...\")\n",
    "    reflection_refinement_record.append([])\n",
    "    prompt_insts.append([])\n",
    "    # sample and update\n",
    "    for index in top_b_indexes:\n",
    "        prompt_insts[i+1].append(prompt_insts[i][index].copy())\n",
    "        error_example_sets = get_error_example_sets_movie(prompt_insts[i][index][\"responses_path\"], ERROR_SAMPLE_NUM, s - 1) \\\n",
    "                            if CURRENT_DATASET == 'movie' \\\n",
    "                            else get_error_example_sets_gsm8k(prompt_insts[i][index][\"responses_path\"], ERROR_SAMPLE_NUM, s - 1)\n",
    "        for error_example_set, set_index in zip(error_example_sets, range(len(error_example_sets))):\n",
    "            reflection_prompt = gen_reflection(FEEDBACK_REASONS_NUM, prompt_insts[i][index][\"inst\"], error_example_set)\n",
    "            reasons = get_reflection_from_optimizer(OPTIMIZER_MODEL, reflection_prompt)\n",
    "            refinement_prompt = gen_refinement(prompt_insts[i][index][\"inst\"], error_example_set, reasons)\n",
    "            improved_inst = get_refinement_from_optimizer(OPTIMIZER_MODEL, refinement_prompt)[0]\n",
    "            prompt_insts[i+1].append({\"inst\": improved_inst, \"accuracy\": 0.0, \"responses_path\": \"\"})\n",
    "            reflection_refinement_record[i].append((reflection_prompt, reasons, refinement_prompt, improved_inst))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prompt instances:\")\n",
    "for iter_list, iter_index in zip(prompt_insts, range(len(prompt_insts))):\n",
    "    print(f\"In iteration {iter_index}: \")\n",
    "    for inst_dict, dict_index in zip(iter_list, range(len(iter_list))):\n",
    "        print(f\">>> {dict_index}\")\n",
    "        print(f\">>> Prompt: {inst_dict['inst']}\")\n",
    "        print(f\">>> Accuracy: {inst_dict['accuracy']}\")\n",
    "        print(f\">>> Responses path: {inst_dict['responses_path']}\")\n",
    "    print(\"=\"*50)\n",
    "print(f\"Best prompt instance: {best_prompt_inst}\")\n",
    "record_save_path = MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR\n",
    "record_save_path += \"/record.txt\"\n",
    "with open(record_save_path, \"w\") as f:\n",
    "    for i in range(len(reflection_refinement_record)):\n",
    "        f.write(f\"\\n\\n>>> Iteration {i}\")\n",
    "        f.write(\"\\n\")\n",
    "        for record in reflection_refinement_record[i]:\n",
    "            f.write(f\">>> Reflection prompt: \\n{record[0]}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\">>> Reasons: \\n{record[1]}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\">>> Refinement prompt: \\n{record[2]}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\">>> Improved prompt: \\n{record[3]}\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "for iter_list in prompt_insts:\n",
    "    accuracy_list.append([inst_dict[\"accuracy\"] for inst_dict in iter_list])\n",
    "print(\"Accuracy list:\")\n",
    "for accuracy_list_item in accuracy_list:\n",
    "    print([round(accuracy, 3) for accuracy in accuracy_list_item])\n",
    "    print(\"average: \", round(sum(accuracy_list_item) / len(accuracy_list_item), 3), \"; max: \", round(max(accuracy_list_item), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" test \"\"\"\n",
    "test_file_path = '../data/gsm8k/test.json'\n",
    "TEST_SAMPLE_NUM = 100\n",
    "test_set = gen_samples_from_dataset(test_file_path, TEST_SAMPLE_NUM, keep_orginal_order=True)\n",
    "test_responses = get_target_model_responses(\n",
    "    CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR, \n",
    "    test_set, best_prompt_inst[\"inst\"], if_print=False\n",
    ")\n",
    "test_responses_path = write_target_model_responses(\n",
    "    TARGET_MODEL + \"_test\",\n",
    "    MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR, \n",
    "    test_responses\n",
    ")\n",
    "accuracy_rate = evaluation_gsm8k(CURRENT_DATASET, test_responses_path)\n",
    "print(f\"Accuracy rate on test set: {accuracy_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" initial test for comparison \"\"\"\n",
    "# test_file_path = '../data/gsm8k/test.json'\n",
    "# TEST_SAMPLE_NUM = 100\n",
    "# test_set = gen_samples_from_dataset(test_file_path, TEST_SAMPLE_NUM, keep_orginal_order=True)\n",
    "# # 0 for seed-0; 0 for seed-1; 1 for seed-2\n",
    "# initial_prompt_inst = prompt_insts[0][1]      # initial max\n",
    "# responses = get_target_model_responses(\n",
    "#     CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR, \n",
    "#     test_set, initial_prompt_inst[\"inst\"], if_print=False\n",
    "# )\n",
    "# responses_path = write_target_model_responses(\n",
    "#     TARGET_MODEL + \"_init-test\",\n",
    "#     MOVIE_RESPONSES_DIR if CURRENT_DATASET == \"movie\" else GSM8K_RESPONSES_DIR, \n",
    "#     responses\n",
    "# )\n",
    "# accuracy_rate = evaluation_gsm8k(CURRENT_DATASET, responses_path)\n",
    "# print(f\"Accuracy rate on test set with initial prompt: {accuracy_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pteng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
