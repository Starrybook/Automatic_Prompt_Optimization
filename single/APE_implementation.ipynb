{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final target: automated prompt engineering\n",
    "def APE_single(dataset, iter_limit, expected_accuracy_rate):\n",
    "    prompt_inst = initial prompt_inst according to dataset\n",
    "    for i in range(iter_limit):\n",
    "        responses = get_responses_from_target_model(prompt_inst)\n",
    "        store responses to file\n",
    "        accuracy_rate = calculate accuracy of responses\n",
    "        if accuracy_rate > expected_accuracy rate:\n",
    "            break\n",
    "        if accuracy_rate has been decreasing for several iterations:\n",
    "            prompt_inst = the hightest one ever occured\n",
    "        error_examples = select error examples from responses\n",
    "        reflection_prompt = get_reflection_prompt(prompt_inst, error_examples)\n",
    "        reasons = reflect(reflection_prompt)\n",
    "        refinement_prompt = get_refinement_prompt(prompt_inst, error_examples, reasons)\n",
    "        improved_prompt = refine(refinement_prompt)\n",
    "        prompt_inst = improved_prompt\n",
    "    return prompt_inst\n",
    "\n",
    "def APE_batch(dataset, iter_limit, expected_accuracy_rate):\n",
    "    prompt_insts = {\"0\": initial prompt_inst according to dataset}\n",
    "    best_prompt_inst = prompt_insts[\"0\"]\n",
    "    for i in range(iter_limit):\n",
    "        responses = {key: get_responses_from_target_model(prompt_inst) for key, prompt_inst in prompt_insts}\n",
    "        store responses to file\n",
    "        for each key in responses:\n",
    "            accuracy_rate = calculate accuracy of responses[key]\n",
    "            update best_prompt_inst if needed\n",
    "            if accuracy_rate > expected_accuracy rate:\n",
    "                break\n",
    "        delete the prompts with bad behavior in prompt_insts\n",
    "        for each key in prompt_insts:\n",
    "            error_examples = select error examples from responses[key]\n",
    "            reflection_prompt = get_reflection_prompt(prompt_insts[key], error_examples)\n",
    "            reasons = reflect(reflection_prompt)\n",
    "            refinement_prompt = get_refinement_prompt(prompt_insts[key], error_examples, reasons)\n",
    "            improved_prompt = refine(refinement_prompt) # {newkey1: ..., newkey2: ......, ......}\n",
    "            prompt_insts += improved_prompt\n",
    "    return best_prompt_inst\n",
    "\"\"\"\n",
    "\"\"\" Globals \"\"\"\n",
    "\"\"\" for APE_utils \"\"\"\n",
    "TARGET_MODEL = \"gpt-3.5-turbo\"\n",
    "OPTIMIZER_MODEL = \"gpt-3.5-turbo\"\n",
    "\"\"\" for data_process \"\"\"\n",
    "TASK_CODE_NAME = 'trial-1'\n",
    "MOVIE_RESPONSES_DIR = \"../data/response/movie\" + '/' + TASK_CODE_NAME\n",
    "GSM8K_RESPONSES_DIR = \"../data/response/gsm8k\" + '/' + TASK_CODE_NAME\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from APE_utils import *\n",
    "\n",
    "# CURRENT_DATASET = \"movie\"\n",
    "CURRENT_DATASET = \"gsm8k\"\n",
    "# eval_file_path = '../data/movie/eval.json'\n",
    "eval_file_path = '../data/gsm8k/eval.json'\n",
    "# if not os.path.exists(MOVIE_RESPONSES_DIR):\n",
    "#     os.makedirs(MOVIE_RESPONSES_DIR)\n",
    "if not os.path.exists(GSM8K_RESPONSES_DIR):\n",
    "    os.makedirs(GSM8K_RESPONSES_DIR)\n",
    "\n",
    "def APE_single(eval_file_path: str, iter_limit: int, expected_accuracy_rate = 1.0):\n",
    "    # set globals\n",
    "    FEEDBACK_REASONS_NUM = 3\n",
    "    FEEDBACK_IMPROVED_PROMPTS_NUM = 1\n",
    "    EVAL_SAMPLE_NUM = 10\n",
    "    ERROR_SAMPLE_NUM = 4\n",
    "    # init\n",
    "    eval_set = gen_samples_from_dataset(eval_file_path, EVAL_SAMPLE_NUM, True)\n",
    "    prompt_inst = MOVIE_INIT_INST if CURRENT_DATASET == 'movie' else GSM8K_INIT_INST\n",
    "    best_prompt_inst = prompt_inst\n",
    "    best_prompt_accuracy_rate = 0.0\n",
    "    early_stop_record_list = []\n",
    "    current_responses_file_path = ''\n",
    "    prompt_inst_id = \"0\"\n",
    "    prompt_inst_list = {prompt_inst_id: prompt_inst}\n",
    "    reason_list = {}\n",
    "    for i in range(iter_limit):\n",
    "        if DEBUG:\n",
    "            print(f\"\\n\\n>>> Current iteration: {i}\")\n",
    "            print(f\">>> Current prompt instance:\\n{prompt_inst}\")\n",
    "        _, current_responses_file_path = get_target_model_responses(\n",
    "            CURRENT_DATASET, TARGET_MODEL, MOVIE_RESPONSES_DIR, GSM8K_RESPONSES_DIR, \n",
    "            eval_set, prompt_inst, if_print=False\n",
    "        )\n",
    "        accuracy_rate = evaluation_movie(CURRENT_DATASET, current_responses_file_path) \\\n",
    "                        if CURRENT_DATASET == 'movie' \\\n",
    "                        else evaluation_gsm8k(CURRENT_DATASET, current_responses_file_path) # will read CURRENT_RESPONSES_FILE_PATH\n",
    "        if DEBUG:\n",
    "            print(f\">>> Current accuracy rate: {accuracy_rate}\")\n",
    "        if accuracy_rate > expected_accuracy_rate:\n",
    "            if DEBUG:\n",
    "                print('>>> Better than expected accuracy rate, stop iteration')\n",
    "            best_prompt_inst = prompt_inst\n",
    "            break\n",
    "        early_stop_record_list.append(accuracy_rate)\n",
    "        if  len(early_stop_record_list) > 3 and \\\n",
    "            early_stop_record_list[-1] < early_stop_record_list[-2] and \\\n",
    "            early_stop_record_list[-2] < early_stop_record_list[-3] and \\\n",
    "            early_stop_record_list[-3] < early_stop_record_list[-4]:\n",
    "            print(f\">>> Early stop triggered at iteration {i}\")\n",
    "            break\n",
    "        best_prompt_inst = prompt_inst if accuracy_rate > best_prompt_accuracy_rate else best_prompt_inst\n",
    "        error_examples = get_error_examples_movie(CURRENT_DATASET, current_responses_file_path, ERROR_SAMPLE_NUM) \\\n",
    "                         if CURRENT_DATASET == 'movie' \\\n",
    "                         else get_error_examples_gsm8k(CURRENT_DATASET, current_responses_file_path, ERROR_SAMPLE_NUM)\n",
    "        reflection_prompt = gen_reflection(FEEDBACK_REASONS_NUM, prompt_inst, error_examples)\n",
    "        if DEBUG:\n",
    "            print(f\"\\n>>> Reflection prompt:\\n{reflection_prompt}\")\n",
    "        reasons = get_reflection_from_optimizer(OPTIMIZER_MODEL, reflection_prompt)\n",
    "        if DEBUG:\n",
    "            print(f\"\\n>>> Reasons:\\n{reasons}\")\n",
    "        reason_list[prompt_inst_id] = reasons\n",
    "        refinement_prompt = gen_refinement(FEEDBACK_IMPROVED_PROMPTS_NUM, prompt_inst, error_examples, reasons)\n",
    "        # if DEBUG:\n",
    "        #     print(f\"\\n>>> Refinement prompt:\\n{refinement_prompt}\")\n",
    "        prompt_inst = get_refinement_from_optimizer(OPTIMIZER_MODEL, refinement_prompt)[0]\n",
    "        prompt_inst_id = str(i+1)\n",
    "        prompt_inst_list[prompt_inst_id] = prompt_inst\n",
    "    print(f\"Best prompt instance: {best_prompt_inst}\")\n",
    "    print(f\"Accuracy rate list: {early_stop_record_list}\")\n",
    "    print(f\"Prompt instance list:\")\n",
    "    for key in prompt_inst_list:\n",
    "        print(f\"Prompt instance {key}: {prompt_inst_list[key]}\")\n",
    "        if key in reason_list:\n",
    "            print(f\"Reasons: {reason_list[key]}\")\n",
    "    return best_prompt_inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APE_single(eval_file_path, 6, 1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pteng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
